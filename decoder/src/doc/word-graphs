-*- mode: text; -*-

The implementation is based on the following paper:

	Stefan Ortmanns and Hermann Ney.  A word graph algorithm for
	large vocabulary continuous speech recognition.  Computer
	Speech and Language (1997) 11, 43-72.



The basic idea.

In our word graph, each node indentifies a (w, f) pair, where w is the
index of a word, and f is the ending frame of the word.  So if there
are two paths in the graph, that both have word w ending on frame f,
the paths must go through the same node n=(w,f).

Each arc in the graph contains two log-probability values: (1) the
total log-prob (language model + acoustic) of the word, and (2) the
acoustic log-prob of the word.  The reason for two values is that,
before using the word graph generated by the recognizer, the use might
want to prune it further using the total log-probabilities.  But, for
example, when rescoring the graph with a new language model, the user
wants to use only the acoustic probabilities of the word graph.

Also, currently we preserve only paths that end in "word_id = <w>"
node in the word graph.



Important note on path probabilities

Because of the word-pair approximation (only the best path between two
words is retained), some of the paths in the word graph may have too
high total log-probability (but not acoustic).  For example, consider
the following paths, where the total negative log-probabilities (the
lower the better) are shown in parenthesis:

	push(1) the(1) button(1)
	bus(1) the(1) button(2)

Because only the best path between words "the" and "button" is
retained, we get the following word graph.

	push(1) --- the(1) --- button(1)
                     |
	bus(1) ------+

Now the path "bus the button" gets too a high total probability, but
of course this is not a problem in rescoring, since only acoustic
log-probabilities are used from the graph anyway.



Implementation in the token-pass decoder

In the decoder, new nodes and arcs are generated only when a token
passes from a last state of a word to the first state of the next
word.  This is accomplished by setting a word_graph_pending flag on,
when the token goes to a node with a word_id, and building word graph
under the following condition:

	(token->word_graph_pending == true) AND
	(target node does not have NODE_AFTER_WORD_ID flag on) AND
	(the transitions was not a self transition)

This considiton assumes:

	* NODE_AFTER_WORD_ID flag is not on when word_id >= 0
	* NODE_AFTER_WORD_ID flag is off on first node of any word
	* NODE_AFTER_WORD_ID is on in word's nodes after (word_id >= 0) node
	
To get access to the relevant nodes in the word graph, the following
information is stored in tokens:

	recent_word_graph_frame: the previous frame that this token
	built word graph.

	recent_word_graph_node: the previous word graph node that this
	token built.

	recent_word_graph_log_prob: the log-probability that this
	token had when it built word graph

Also, the decoder maintains a vector, that contains a pair
(word_graph_node, frame) for each possible word_id.  In this vector we
can store the word graph node that corresponds to a certain word_id on
the current frame.  The frame is stored only for detecting if a node
has been created for a certain word_id on the *current* frame.



Pruning

During the recognition, the dead ends generated by dying tokens are
pruned away using reference counting on word graph nodes.  Otherwise,
the word graph is not pruned.  In the end, we prune away paths that do
not end in the <w> node of the word graph.  The resulting word graphs
are quite big, but it is easy to prune them with a finite state
automata tools before further use.



Other notes

Note, that if you use 2-grams, you normally would want to set
prune_similar to 1, but word graph algorithm requires that the
threshold is at least two.  Otherwise the word graphs contain only one
path.
